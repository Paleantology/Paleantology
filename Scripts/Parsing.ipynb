{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do in this notebook is prepare data from three sources (molecular, morphological and fossils without data, used only for their age information) for use with the BEAST2 suite of tools for divergence dating using the Fossilized Birth-Death model. BEAST2 has a requirement that can make this challenging: all taxa have to be present in all blocks (i.e., your morphology block has to have all your morphology and molecular taxa). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is load our the two libraries we'll be using: Pandas, a data-crunching library and Dendropy, a perennially excellent package for phylogenetic tree and dataset manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dendropy\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to have us start out by processing the fossils that don't have associated morphological data. These fossils will have missing data for every cell in the phylogenetic matrix. We just want them for their age information. The cool thing about using a fossilized birth-death model is that you can use as many specimens as you have - making complete use of the data is one advantage of this method over the traditional node calibration framework.\n",
    "\n",
    "For this work, I have obtained some ant fossil occurances from PaleoDB. Let's read them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbdb_raw = pd.read_csv(\"../Data/Morph/Raw/pbdb_data.csv\", skiprows=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of data in this, which I don't really want to deal with right now. So I'm going to slim it down to a few important columns (the name, reference minimum and maximum ages) and drop any duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbdb_slim = pbdb_raw[['accepted_name', 'reference_no', 'max_ma', 'min_ma']]\n",
    "pbdb_nodup = pbdb_slim.drop_duplicates(subset='accepted_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbdb_nodup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has 789 rows, down from ~1800. That being said, the quality assurance on them may be quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to prepare the data, I want to add some columns. In all of my files, I keep track of the higher taxonomy of the ants. In the future, I will want to be able to parse these files to subsample my ants by family, tribe or genus to look at issues of fossil sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbdb_nodup['SubFamily'] = 'None'\n",
    "pbdb_nodup['Tribe'] = 'None'\n",
    "pbdb_nodup['Genus'] = 'None'\n",
    "pbdb_nodup['Fossil'] = 'Yes'\n",
    "pbdb_nodup = pbdb_nodup[['accepted_name', 'reference_no', 'SubFamily','Tribe','Genus','Fossil','min_ma','max_ma']]\n",
    "pbdb_nodup['Notes'] = 'Note'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know this information yet, so I'm initializing these columns without data. A couple more housekeeping steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pbdb_nodup.accepted_name= pbdb_nodup.accepted_name.str.replace(' ','_')\n",
    "pbdb_sort = pbdb_nodup.sort(columns='accepted_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted each of the names to have an underscore in place of a space. Most phylogeny programs don't deal well with spaces. I also wanted to sort by alphabet, just because that's nice to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " pbdb_nodup.loc[pbdb_nodup.accepted_name.str.split('_').str[0].str[-3:] == 'nae', 'SubFamily'] = pbdb_nodup.accepted_name\n",
    " pbdb_nodup.loc[pbdb_nodup.accepted_name.str.split('_').str[0].str[-3:] != 'nae', 'Genus'] = pbdb_nodup.accepted_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data operation will fill in some of those missing columns. If the last three letters of a name are 'nae', we know that that's a subfamily. So we can put that name in the subfamily column. If it ends in anything else, we know it's not a subfamily and is a genus.\n",
    "\n",
    "Because some of these groups are extinct, they don't appear in my other taxonomies, which are of extants. So this matrix is still really sparse. I populated the rest by hand.\n",
    "\n",
    "BEAST reads tip dates for fossils off of the end of your taxon labels. The below joins together the taxon name with its age info. If you don't want that, skip to the cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tax_labels = [tax+'_'+str(min)+'_'+str(max) for tax,min,max in zip(pbdb_nodup.accepted_name,pbdb_nodup.min_ma,pbdb_nodup.max_ma)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tax_labels = pbdb_nodup.accepted_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned in the intro, BEAST wants data for every taxon for each data type. So we can create a dictionary of the taxon names and the missing data symbol for those taxa that don't have data associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md_val = \"?\"*139\n",
    "dict_of_dat = {}\n",
    "for name in tax_labels:                                                                                             \n",
    "\tdict_of_dat[name] = md_val\n",
    "dict_of_moldat = {}\n",
    "md_val = \"?\"*4572\n",
    "for name in tax_labels:                                                                                             \n",
    "\tdict_of_moldat[name] = md_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in the molecular and morphological data. Because BEAST needs all data partitions to have the same taxon names, we want these to have the same taxon namespace, or the same expected taxon names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxa = dendropy.TaxonNamespace()                                                                                      \n",
    "morphDat = dendropy.StandardCharacterMatrix.get_from_path(\"../Data/Morph/KellerMatrix.nex\", schema=\"nexus\", taxon_namespace=taxa)\n",
    "molDat = dendropy.DnaCharacterMatrix.get_from_path(\"../Data/Mol/Moreau_simple.nex\", schema=\"nexus\", taxon_namespace=taxa)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we're going to make dataset objects to turn our dictionaries of data into actual phylogenetic matrices. We're doing this so that they're in the taxon namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fossDat = dendropy.StandardCharacterMatrix.from_dict(dict_of_dat,taxon_namespace=taxa)    \t\n",
    "fossmolDat = dendropy.DnaCharacterMatrix.from_dict(dict_of_moldat, taxon_namespace=taxa) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of our taxa with DNA don't have morphological data, and vice versa, we need to pack those taxa with missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morphDat.pack()\n",
    "molDat.pack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have two matrices, with all of our taxa (1027) represented for both molecular data and morphological data. And then we can write them out. These are prepared to load into BEAST  - though pack uses 'None' as missing data. You can fix this with grep to whatever your missing data character is (for phylogenetics, this is a question mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fossDat.write_to_path('samp_morphTest.nex', schema='nexus')\n",
    "fossmolDat.write_to_path('samp_molTest.nex', schema='nexus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have matrices of characters. The next thing we might want to do is get some taxon sets, which are groupings of taxa that we can tell BEAST to make monophyletic and keep track of. In the traditional node calibration framework, people would put a prior on the age of each taxon set. Because we are using FBD dating, the ages of all the fossils in that set can be used to date that group. \n",
    "\n",
    "I'm going to make taxon sets at the subfamily level. I have taxonomies for all the data I have molecular data for, for my morphology taxa and for my calibration taxa. I want to read in those taxonomies, and use this to group taxa by subfamily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morphTable = pd.read_csv(\"../Data/Morph/morphTNRS.csv\")\n",
    "molTable = pd.read_csv(\"../Data/Mol/molTNRS.csv\")\n",
    "fossTable = pd.read_csv(\"../Data/Morph/FossilTNRS.csv\")\n",
    "\n",
    "\n",
    "fossMerge = fossTable[['accepted_name','subfamily']] \n",
    "fossMerge.columns = ['specimen','subfamily'] \n",
    "fossMerge['Fossil'] = 'Yes'\n",
    "morphMerge = morphTable[['Specimen', 'SubFamily']]\n",
    "morphMerge.columns = ['specimen','subfamily'] \n",
    "morphMerge['Fossil'] = 'No'\n",
    "molMerge = molTable[['Moreau_et_al_name', 'subfamily']]\n",
    "molMerge.columns = ['specimen','subfamily']  \n",
    "molMerge['Fossil'] = 'No'\n",
    "mega_df = pd.concat([fossMerge,molMerge,morphMerge]) \n",
    "mega_df = mega_df.drop_duplicates('specimen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we read in the files of taxonomies, pulled the relevant data, made sure all the columns had the same name, and then concatenated them into one mega matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "families = mega_df.groupby('subfamily')\n",
    "\n",
    "for name, group in families:\n",
    "     print(name)\n",
    "     print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code groups the ants by subfamily, and then prints all members of the subfamily to the screen. In BEAUTI's taxon set panel, you can either enter these groupings by hand, or you can print them to a file and parse them into XML. You can look at an individual group like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "families.get_group('Myrmicinae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if you wanted to export your group to a file so you can assemble a BEAST XML by hand, you could export the taxon set like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, group in families:\n",
    "    fname = name\n",
    "    group.specimen.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the three main components of the input BEAST needs: taxon names, datasets and taxon sets. Parsing all this output can be a little daunting, so hopefully this was useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
